{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "073be405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee5989b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f45170bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 20\n",
    "DEVICE = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f17386c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize='spacy', \n",
    "    tokenizer_language=\"en_core_web_sm\",\n",
    "    include_lengths=True # NEW\n",
    ")\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0367a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
    "\n",
    "dataset1 = torchtext.legacy.data.TabularDataset(\n",
    "    path='train.csv', format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69ab2c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 34197\n",
      "Num Test: 3\n"
     ]
    }
   ],
   "source": [
    "train_data, train_data_dum = dataset1.split(\n",
    "    split_ratio=[0.9999, 0.0001],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(train_data_dum)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ca726c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "# print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "527ea0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_loader_dum = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, train_data_dum), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        sort_within_batch=True, # NEW. necessary for packed_padded_sequence\n",
    "             sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    "        device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d1a5f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text matrix size: torch.Size([8, 100])\n",
      "Target vector size: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME[0].size()}')\n",
    "    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7b941e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # ebedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        ## NEW\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96624b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98e95e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "            # NEW\n",
    "            features, text_length = batch_data.TEXT_COLUMN_NAME\n",
    "            targets = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n",
    "            \n",
    "            logits = model(features, text_length)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10daf55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 000/342 | Loss: 3.4037\n",
      "Epoch: 001/020 | Batch 050/342 | Loss: 3.2059\n",
      "Epoch: 001/020 | Batch 100/342 | Loss: 2.8130\n",
      "Epoch: 001/020 | Batch 150/342 | Loss: 2.7385\n",
      "Epoch: 001/020 | Batch 200/342 | Loss: 2.1630\n",
      "Epoch: 001/020 | Batch 250/342 | Loss: 2.5236\n",
      "Epoch: 001/020 | Batch 300/342 | Loss: 1.9429\n",
      "training accuracy: 51.09%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 002/020 | Batch 000/342 | Loss: 1.8352\n",
      "Epoch: 002/020 | Batch 050/342 | Loss: 2.3981\n",
      "Epoch: 002/020 | Batch 100/342 | Loss: 1.7239\n",
      "Epoch: 002/020 | Batch 150/342 | Loss: 1.4464\n",
      "Epoch: 002/020 | Batch 200/342 | Loss: 1.6628\n",
      "Epoch: 002/020 | Batch 250/342 | Loss: 1.5371\n",
      "Epoch: 002/020 | Batch 300/342 | Loss: 1.4687\n",
      "training accuracy: 75.67%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 1.72 min\n",
      "Epoch: 003/020 | Batch 000/342 | Loss: 0.8734\n",
      "Epoch: 003/020 | Batch 050/342 | Loss: 0.9062\n",
      "Epoch: 003/020 | Batch 100/342 | Loss: 0.7012\n",
      "Epoch: 003/020 | Batch 150/342 | Loss: 0.8289\n",
      "Epoch: 003/020 | Batch 200/342 | Loss: 0.8986\n",
      "Epoch: 003/020 | Batch 250/342 | Loss: 0.9551\n",
      "Epoch: 003/020 | Batch 300/342 | Loss: 1.0604\n",
      "training accuracy: 89.14%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 2.54 min\n",
      "Epoch: 004/020 | Batch 000/342 | Loss: 0.8641\n",
      "Epoch: 004/020 | Batch 050/342 | Loss: 0.4829\n",
      "Epoch: 004/020 | Batch 100/342 | Loss: 0.2835\n",
      "Epoch: 004/020 | Batch 150/342 | Loss: 0.6983\n",
      "Epoch: 004/020 | Batch 200/342 | Loss: 0.4936\n",
      "Epoch: 004/020 | Batch 250/342 | Loss: 0.4908\n",
      "Epoch: 004/020 | Batch 300/342 | Loss: 0.4350\n",
      "training accuracy: 95.83%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 3.37 min\n",
      "Epoch: 005/020 | Batch 000/342 | Loss: 0.2137\n",
      "Epoch: 005/020 | Batch 050/342 | Loss: 0.0651\n",
      "Epoch: 005/020 | Batch 100/342 | Loss: 0.1147\n",
      "Epoch: 005/020 | Batch 150/342 | Loss: 0.1256\n",
      "Epoch: 005/020 | Batch 200/342 | Loss: 0.1077\n",
      "Epoch: 005/020 | Batch 250/342 | Loss: 0.4931\n",
      "Epoch: 005/020 | Batch 300/342 | Loss: 0.2378\n",
      "training accuracy: 98.38%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 4.20 min\n",
      "Epoch: 006/020 | Batch 000/342 | Loss: 0.0605\n",
      "Epoch: 006/020 | Batch 050/342 | Loss: 0.2542\n",
      "Epoch: 006/020 | Batch 100/342 | Loss: 0.1182\n",
      "Epoch: 006/020 | Batch 150/342 | Loss: 0.0692\n",
      "Epoch: 006/020 | Batch 200/342 | Loss: 0.2199\n",
      "Epoch: 006/020 | Batch 250/342 | Loss: 0.1221\n",
      "Epoch: 006/020 | Batch 300/342 | Loss: 0.0457\n",
      "training accuracy: 99.25%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 5.03 min\n",
      "Epoch: 007/020 | Batch 000/342 | Loss: 0.0447\n",
      "Epoch: 007/020 | Batch 050/342 | Loss: 0.0171\n",
      "Epoch: 007/020 | Batch 100/342 | Loss: 0.0153\n",
      "Epoch: 007/020 | Batch 150/342 | Loss: 0.0483\n",
      "Epoch: 007/020 | Batch 200/342 | Loss: 0.0486\n",
      "Epoch: 007/020 | Batch 250/342 | Loss: 0.1425\n",
      "Epoch: 007/020 | Batch 300/342 | Loss: 0.0658\n",
      "training accuracy: 99.46%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 5.86 min\n",
      "Epoch: 008/020 | Batch 000/342 | Loss: 0.0085\n",
      "Epoch: 008/020 | Batch 050/342 | Loss: 0.0044\n",
      "Epoch: 008/020 | Batch 100/342 | Loss: 0.0082\n",
      "Epoch: 008/020 | Batch 150/342 | Loss: 0.0123\n",
      "Epoch: 008/020 | Batch 200/342 | Loss: 0.0074\n",
      "Epoch: 008/020 | Batch 250/342 | Loss: 0.0115\n",
      "Epoch: 008/020 | Batch 300/342 | Loss: 0.0088\n",
      "training accuracy: 99.51%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 6.69 min\n",
      "Epoch: 009/020 | Batch 000/342 | Loss: 0.0040\n",
      "Epoch: 009/020 | Batch 050/342 | Loss: 0.0036\n",
      "Epoch: 009/020 | Batch 100/342 | Loss: 0.0048\n",
      "Epoch: 009/020 | Batch 150/342 | Loss: 0.0230\n",
      "Epoch: 009/020 | Batch 200/342 | Loss: 0.0070\n",
      "Epoch: 009/020 | Batch 250/342 | Loss: 0.0052\n",
      "Epoch: 009/020 | Batch 300/342 | Loss: 0.0096\n",
      "training accuracy: 99.55%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 7.55 min\n",
      "Epoch: 010/020 | Batch 000/342 | Loss: 0.0041\n",
      "Epoch: 010/020 | Batch 050/342 | Loss: 0.0082\n",
      "Epoch: 010/020 | Batch 100/342 | Loss: 0.0053\n",
      "Epoch: 010/020 | Batch 150/342 | Loss: 0.0041\n",
      "Epoch: 010/020 | Batch 200/342 | Loss: 0.0049\n",
      "Epoch: 010/020 | Batch 250/342 | Loss: 0.0957\n",
      "Epoch: 010/020 | Batch 300/342 | Loss: 0.0025\n",
      "training accuracy: 99.52%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 8.38 min\n",
      "Epoch: 011/020 | Batch 000/342 | Loss: 0.0017\n",
      "Epoch: 011/020 | Batch 050/342 | Loss: 0.0028\n",
      "Epoch: 011/020 | Batch 100/342 | Loss: 0.0024\n",
      "Epoch: 011/020 | Batch 150/342 | Loss: 0.0066\n",
      "Epoch: 011/020 | Batch 200/342 | Loss: 0.0138\n",
      "Epoch: 011/020 | Batch 250/342 | Loss: 0.0690\n",
      "Epoch: 011/020 | Batch 300/342 | Loss: 0.1579\n",
      "training accuracy: 90.47%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 9.21 min\n",
      "Epoch: 012/020 | Batch 000/342 | Loss: 0.3957\n",
      "Epoch: 012/020 | Batch 050/342 | Loss: 0.6058\n",
      "Epoch: 012/020 | Batch 100/342 | Loss: 1.2974\n",
      "Epoch: 012/020 | Batch 150/342 | Loss: 0.7942\n",
      "Epoch: 012/020 | Batch 200/342 | Loss: 1.0365\n",
      "Epoch: 012/020 | Batch 250/342 | Loss: 0.9850\n",
      "Epoch: 012/020 | Batch 300/342 | Loss: 0.9328\n",
      "training accuracy: 88.38%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 10.04 min\n",
      "Epoch: 013/020 | Batch 000/342 | Loss: 0.3483\n",
      "Epoch: 013/020 | Batch 050/342 | Loss: 0.4098\n",
      "Epoch: 013/020 | Batch 100/342 | Loss: 0.4111\n",
      "Epoch: 013/020 | Batch 150/342 | Loss: 0.3788\n",
      "Epoch: 013/020 | Batch 200/342 | Loss: 0.4435\n",
      "Epoch: 013/020 | Batch 250/342 | Loss: 0.4216\n",
      "Epoch: 013/020 | Batch 300/342 | Loss: 0.6827\n",
      "training accuracy: 97.09%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 10.88 min\n",
      "Epoch: 014/020 | Batch 000/342 | Loss: 0.0683\n",
      "Epoch: 014/020 | Batch 050/342 | Loss: 0.1749\n",
      "Epoch: 014/020 | Batch 100/342 | Loss: 0.0777\n",
      "Epoch: 014/020 | Batch 150/342 | Loss: 0.0850\n",
      "Epoch: 014/020 | Batch 200/342 | Loss: 0.1298\n",
      "Epoch: 014/020 | Batch 250/342 | Loss: 0.1700\n",
      "Epoch: 014/020 | Batch 300/342 | Loss: 0.1469\n",
      "training accuracy: 99.29%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 11.72 min\n",
      "Epoch: 015/020 | Batch 000/342 | Loss: 0.0481\n",
      "Epoch: 015/020 | Batch 050/342 | Loss: 0.0307\n",
      "Epoch: 015/020 | Batch 100/342 | Loss: 0.0205\n",
      "Epoch: 015/020 | Batch 150/342 | Loss: 0.0199\n",
      "Epoch: 015/020 | Batch 200/342 | Loss: 0.0202\n",
      "Epoch: 015/020 | Batch 250/342 | Loss: 0.0236\n",
      "Epoch: 015/020 | Batch 300/342 | Loss: 0.0180\n",
      "training accuracy: 99.57%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 12.56 min\n",
      "Epoch: 016/020 | Batch 000/342 | Loss: 0.0055\n",
      "Epoch: 016/020 | Batch 050/342 | Loss: 0.0098\n",
      "Epoch: 016/020 | Batch 100/342 | Loss: 0.0229\n",
      "Epoch: 016/020 | Batch 150/342 | Loss: 0.0057\n",
      "Epoch: 016/020 | Batch 200/342 | Loss: 0.0038\n",
      "Epoch: 016/020 | Batch 250/342 | Loss: 0.0082\n",
      "Epoch: 016/020 | Batch 300/342 | Loss: 0.0071\n",
      "training accuracy: 99.62%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 13.39 min\n",
      "Epoch: 017/020 | Batch 000/342 | Loss: 0.0015\n",
      "Epoch: 017/020 | Batch 050/342 | Loss: 0.0071\n",
      "Epoch: 017/020 | Batch 100/342 | Loss: 0.0044\n",
      "Epoch: 017/020 | Batch 150/342 | Loss: 0.0024\n",
      "Epoch: 017/020 | Batch 200/342 | Loss: 0.0036\n",
      "Epoch: 017/020 | Batch 250/342 | Loss: 0.0014\n",
      "Epoch: 017/020 | Batch 300/342 | Loss: 0.0082\n",
      "training accuracy: 99.63%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 14.22 min\n",
      "Epoch: 018/020 | Batch 000/342 | Loss: 0.0034\n",
      "Epoch: 018/020 | Batch 050/342 | Loss: 0.0231\n",
      "Epoch: 018/020 | Batch 100/342 | Loss: 0.0119\n",
      "Epoch: 018/020 | Batch 150/342 | Loss: 0.0022\n",
      "Epoch: 018/020 | Batch 200/342 | Loss: 0.0727\n",
      "Epoch: 018/020 | Batch 250/342 | Loss: 0.0017\n",
      "Epoch: 018/020 | Batch 300/342 | Loss: 0.0017\n",
      "training accuracy: 99.64%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 15.05 min\n",
      "Epoch: 019/020 | Batch 000/342 | Loss: 0.0030\n",
      "Epoch: 019/020 | Batch 050/342 | Loss: 0.0012\n",
      "Epoch: 019/020 | Batch 100/342 | Loss: 0.0026\n",
      "Epoch: 019/020 | Batch 150/342 | Loss: 0.0023\n",
      "Epoch: 019/020 | Batch 200/342 | Loss: 0.0012\n",
      "Epoch: 019/020 | Batch 250/342 | Loss: 0.0362\n",
      "Epoch: 019/020 | Batch 300/342 | Loss: 0.0012\n",
      "training accuracy: 99.64%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 15.88 min\n",
      "Epoch: 020/020 | Batch 000/342 | Loss: 0.0137\n",
      "Epoch: 020/020 | Batch 050/342 | Loss: 0.0028\n",
      "Epoch: 020/020 | Batch 100/342 | Loss: 0.0006\n",
      "Epoch: 020/020 | Batch 150/342 | Loss: 0.0037\n",
      "Epoch: 020/020 | Batch 200/342 | Loss: 0.0033\n",
      "Epoch: 020/020 | Batch 250/342 | Loss: 0.0253\n",
      "Epoch: 020/020 | Batch 300/342 | Loss: 0.0012\n",
      "training accuracy: 99.64%\n",
      "valid accuracy: 33.33%\n",
      "Time elapsed: 16.71 min\n",
      "Total Training Time: 16.71 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        # NEW\n",
    "        features, text_length = batch_data.TEXT_COLUMN_NAME\n",
    "        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(features, text_length)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader_dum, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "547c26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test= pd.read_csv('non_comp_test_x.csv')\n",
    "# y_test=pd.read_csv('non_comp_test_y.csv')\n",
    "\n",
    "# data=pd.concat([X_test['Title'],y_test['Genre']],axis=1)\n",
    "# data.columns=['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
    "# data.to_csv('test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b3e11b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = torchtext.legacy.data.TabularDataset(\n",
    "    path='test.csv', format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7442d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 5699\n",
      "Num Test: 1\n"
     ]
    }
   ],
   "source": [
    "test_data, test_data_dum = dataset2.split(\n",
    "    split_ratio=[0.9999, 0.0001],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(test_data)}')\n",
    "print(f'Num Test: {len(test_data_dum)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "647076d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader, test_loader_dum = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (test_data, test_data_dum), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        sort_within_batch=True, # NEW. necessary for packed_padded_sequence\n",
    "             sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    "        device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "edb589c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.04%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3e99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
