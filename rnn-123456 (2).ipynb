{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport torchtext\nimport torch.nn.functional as F\nimport random\nimport torch\nimport spacy\nimport re\nfrom collections import Counter\nimport string\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-27T07:10:35.827495Z","iopub.execute_input":"2022-11-27T07:10:35.828724Z","iopub.status.idle":"2022-11-27T07:10:35.835974Z","shell.execute_reply.started":"2022-11-27T07:10:35.828671Z","shell.execute_reply":"2022-11-27T07:10:35.834585Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#this will be used for tokenizing \ntok = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.035220Z","iopub.execute_input":"2022-11-27T07:10:36.035630Z","iopub.status.idle":"2022-11-27T07:10:36.726557Z","shell.execute_reply.started":"2022-11-27T07:10:36.035597Z","shell.execute_reply":"2022-11-27T07:10:36.725453Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"X_train=pd.read_csv('../input/col774-2022/train_x.csv')[['Title']]\ny_train=pd.read_csv('../input/col774-2022/train_y.csv')['Genre']\n\nX_test=pd.read_csv('../input/col774-2022/non_comp_test_x.csv')[['Title']]\ny_test=pd.read_csv('../input/col774-2022/non_comp_test_y.csv')['Genre']\n# test= pd.concat([X_test[['Id','Title']],y_test['Genre']],axis=1)\n\nX_test_comp=pd.read_csv('../input/col774-2022/comp_test_x.csv')\ncomp_test=X_test_comp[['Id','Title']]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.728352Z","iopub.execute_input":"2022-11-27T07:10:36.728677Z","iopub.status.idle":"2022-11-27T07:10:36.847218Z","shell.execute_reply.started":"2022-11-27T07:10:36.728648Z","shell.execute_reply":"2022-11-27T07:10:36.845440Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# X_train=pd.read_csv('../input/col774-2022/train_x.csv')[['Title']]\n# y_train=pd.read_csv('../input/col774-2022/train_y.csv')\n\n# X_test=pd.read_csv('../input/col774-2022/non_comp_test_x.csv')[['Title']]\n# y_test=pd.read_csv('../input/col774-2022/non_comp_test_y.csv')\n# # test= pd.concat([X_test[['Id','Title']],y_test['Genre']],axis=1)\n\n# X_test_comp=pd.read_csv('../input/col774-2022/comp_test_x.csv')\n# comp_test=X_test_comp[['Id','Title']]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.848645Z","iopub.execute_input":"2022-11-27T07:10:36.849031Z","iopub.status.idle":"2022-11-27T07:10:36.854703Z","shell.execute_reply.started":"2022-11-27T07:10:36.848997Z","shell.execute_reply":"2022-11-27T07:10:36.853507Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"X_train['Title'].loc[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.857115Z","iopub.execute_input":"2022-11-27T07:10:36.857465Z","iopub.status.idle":"2022-11-27T07:10:36.869581Z","shell.execute_reply.started":"2022-11-27T07:10:36.857434Z","shell.execute_reply":"2022-11-27T07:10:36.868544Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"\"Lutheran and Catholic Reconciliation on Justification: A Chronology of the Holy See's Contributions, 1961-1999, to a New Relationship ; between ... Declaration on the Doctrine of Justification\""},"metadata":{}}]},{"cell_type":"code","source":"def normalize(text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n    text = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(text)]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.871314Z","iopub.execute_input":"2022-11-27T07:10:36.871734Z","iopub.status.idle":"2022-11-27T07:10:36.881619Z","shell.execute_reply.started":"2022-11-27T07:10:36.871632Z","shell.execute_reply":"2022-11-27T07:10:36.880764Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"#count number of occurences of each word\nfrequency = Counter()\nfor index, row in X_train.iterrows():\n    frequency.update(normalize(row['Title']))\n    \n#deleting iwords with frequency one (optional)\nfor word in list(frequency):\n    if frequency[word] < 2:\n        del frequency[word]\n# print(\"num_words after:\",len(frequency.keys()))\n\n#creating mapping\nvocab_index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in frequency:\n    vocab_index[word] = len(words)\n    words.append(word)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:36.882685Z","iopub.execute_input":"2022-11-27T07:10:36.883027Z","iopub.status.idle":"2022-11-27T07:10:44.090596Z","shell.execute_reply.started":"2022-11-27T07:10:36.882998Z","shell.execute_reply":"2022-11-27T07:10:44.089387Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def words_to_numbers(text, vocab_index, N=35):\n    text= normalize(text)\n    enc1 = np.zeros(N, dtype=int)\n    i=0\n    for word in text:\n        if word in vocab_index:\n            enc1[i]=vocab_index[word]\n        else:\n            enc1[i]=vocab_index[\"UNK\"]\n        i=i+1\n        if i>=N:\n            break\n    \n    return enc1","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:44.092136Z","iopub.execute_input":"2022-11-27T07:10:44.092811Z","iopub.status.idle":"2022-11-27T07:10:44.100122Z","shell.execute_reply.started":"2022-11-27T07:10:44.092768Z","shell.execute_reply":"2022-11-27T07:10:44.099276Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def preprocess(data,vocab_index):\n    data['feature']= data['Title'].apply(lambda x: np.array(words_to_numbers(x,vocab_index )))\n    data.drop('Title',axis=1,inplace=True)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:44.102305Z","iopub.execute_input":"2022-11-27T07:10:44.103310Z","iopub.status.idle":"2022-11-27T07:10:44.111424Z","shell.execute_reply.started":"2022-11-27T07:10:44.103263Z","shell.execute_reply":"2022-11-27T07:10:44.110299Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"preprocess(X_train,vocab_index)\npreprocess(X_test,vocab_index)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:44.112823Z","iopub.execute_input":"2022-11-27T07:10:44.113352Z","iopub.status.idle":"2022-11-27T07:10:47.013652Z","shell.execute_reply.started":"2022-11-27T07:10:44.113304Z","shell.execute_reply":"2022-11-27T07:10:47.012486Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                                                feature\n0     [2904, 3, 21, 3463, 1627, 8, 1453, 957, 100, 6...\n1     [1, 4797, 8, 12, 21, 278, 516, 22, 821, 3, 12,...\n2     [12, 662, 11830, 8, 12, 21, 1832, 241, 11, 100...\n3     [247, 5920, 11, 8356, 8, 816, 21, 123, 15, 233...\n4     [6365, 622, 623, 21, 331, 8, 6371, 6355, 8, 63...\n...                                                 ...\n5695  [21, 3397, 8819, 3351, 1457, 0, 0, 0, 0, 0, 0,...\n5696  [8214, 67, 9, 1284, 393, 0, 0, 0, 0, 0, 0, 0, ...\n5697  [1348, 931, 4339, 67, 8727, 0, 0, 0, 0, 0, 0, ...\n5698  [50, 8, 1, 153, 250, 8, 1, 8, 1, 742, 67, 3770...\n5699  [453, 985, 21, 240, 2635, 144, 60, 8, 3584, 21...\n\n[5700 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2904, 3, 21, 3463, 1627, 8, 1453, 957, 100, 6...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 4797, 8, 12, 21, 278, 516, 22, 821, 3, 12,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[12, 662, 11830, 8, 12, 21, 1832, 241, 11, 100...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[247, 5920, 11, 8356, 8, 816, 21, 123, 15, 233...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[6365, 622, 623, 21, 331, 8, 6371, 6355, 8, 63...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5695</th>\n      <td>[21, 3397, 8819, 3351, 1457, 0, 0, 0, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>5696</th>\n      <td>[8214, 67, 9, 1284, 393, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5697</th>\n      <td>[1348, 931, 4339, 67, 8727, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5698</th>\n      <td>[50, 8, 1, 153, 250, 8, 1, 8, 1, 742, 67, 3770...</td>\n    </tr>\n    <tr>\n      <th>5699</th>\n      <td>[453, 985, 21, 240, 2635, 144, 60, 8, 3584, 21...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5700 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class TitleDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.X[idx].astype(np.int32)), self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:47.015418Z","iopub.execute_input":"2022-11-27T07:10:47.015737Z","iopub.status.idle":"2022-11-27T07:10:47.022633Z","shell.execute_reply.started":"2022-11-27T07:10:47.015709Z","shell.execute_reply":"2022-11-27T07:10:47.021604Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"\nX_train=list(X_train['feature'])\ny_train=list(y_train)\nX_test=list(X_test['feature'])\ny_test=list(y_test)\n\ntrain_dataset = TitleDataset(X_train, y_train)\ntest_dataset = TitleDataset(X_test, y_test)\n\nbatch_size = 100\nvocab_size = len(words)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:10:47.023790Z","iopub.execute_input":"2022-11-27T07:10:47.024122Z","iopub.status.idle":"2022-11-27T07:10:47.044746Z","shell.execute_reply.started":"2022-11-27T07:10:47.024092Z","shell.execute_reply":"2022-11-27T07:10:47.043631Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nclass RNN(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embedding =torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = torch.nn.Linear(hidden_dim, 30)\n        self.dropout = torch.nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.rnn(x)\n        output= self.linear(ht[-1])\n        return output\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:44:25.601647Z","iopub.execute_input":"2022-11-27T07:44:25.602095Z","iopub.status.idle":"2022-11-27T07:44:25.612587Z","shell.execute_reply.started":"2022-11-27T07:44:25.602061Z","shell.execute_reply":"2022-11-27T07:44:25.611366Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def compute_accuracy(model, test_loader):\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    sum_rmse = 0.0\n    for x, y in test_loader:\n        x = x.long()\n        y = y.long()\n        y_pred = model(x)\n        loss = F.cross_entropy(y_pred, y)\n        pred = torch.max(y_pred, 1)[1]   \n        correct += (pred == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total\n\ndef train_model(model, epochs=20, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        print(f'Epoch:{i}')\n        sum_loss = 0.0\n        total = 0\n        for x, y in train_loader:\n            x = x.long()\n            y = y.long()\n            y_pred = model(x)\n            optimizer.zero_grad()\n            loss = F.cross_entropy(y_pred, y)\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        test_loss, test_acc = compute_accuracy(model, test_loader)\n        print(\"train loss %.3f, test loss %.3f, test accuracy %.3f\" % (sum_loss/total, test_loss, test_acc*100))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:44:26.135712Z","iopub.execute_input":"2022-11-27T07:44:26.136369Z","iopub.status.idle":"2022-11-27T07:44:26.148734Z","shell.execute_reply.started":"2022-11-27T07:44:26.136323Z","shell.execute_reply":"2022-11-27T07:44:26.147420Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"model =  RNN(vocab_size, 128, 128)\ntrain_model(model, epochs=30, lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:44:34.568298Z","iopub.execute_input":"2022-11-27T07:44:34.568704Z","iopub.status.idle":"2022-11-27T07:53:28.040406Z","shell.execute_reply.started":"2022-11-27T07:44:34.568674Z","shell.execute_reply":"2022-11-27T07:53:28.038910Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Epoch:0\ntrain loss 3.405, test loss 3.395, test accuracy 4.246\nEpoch:1\ntrain loss 3.100, test loss 2.862, test accuracy 16.263\nEpoch:2\ntrain loss 2.659, test loss 2.637, test accuracy 22.877\nEpoch:3\ntrain loss 2.384, test loss 2.529, test accuracy 27.895\nEpoch:4\ntrain loss 2.185, test loss 2.422, test accuracy 31.404\nEpoch:5\ntrain loss 2.027, test loss 2.361, test accuracy 33.509\nEpoch:6\ntrain loss 1.901, test loss 2.317, test accuracy 34.825\nEpoch:7\ntrain loss 1.805, test loss 2.310, test accuracy 36.807\nEpoch:8\ntrain loss 1.722, test loss 2.305, test accuracy 37.421\nEpoch:9\ntrain loss 1.653, test loss 2.277, test accuracy 38.632\nEpoch:10\ntrain loss 1.592, test loss 2.280, test accuracy 39.000\nEpoch:11\ntrain loss 1.532, test loss 2.286, test accuracy 39.298\nEpoch:12\ntrain loss 1.478, test loss 2.254, test accuracy 40.860\nEpoch:13\ntrain loss 1.432, test loss 2.286, test accuracy 40.596\nEpoch:14\ntrain loss 1.394, test loss 2.289, test accuracy 41.158\nEpoch:15\ntrain loss 1.355, test loss 2.296, test accuracy 40.930\nEpoch:16\ntrain loss 1.318, test loss 2.283, test accuracy 41.825\nEpoch:17\ntrain loss 1.288, test loss 2.308, test accuracy 41.842\nEpoch:18\ntrain loss 1.265, test loss 2.306, test accuracy 42.386\nEpoch:19\ntrain loss 1.234, test loss 2.341, test accuracy 42.123\nEpoch:20\ntrain loss 1.220, test loss 2.321, test accuracy 42.316\nEpoch:21\ntrain loss 1.188, test loss 2.358, test accuracy 42.000\nEpoch:22\ntrain loss 1.156, test loss 2.354, test accuracy 42.825\nEpoch:23\ntrain loss 1.165, test loss 2.337, test accuracy 42.684\nEpoch:24\ntrain loss 1.123, test loss 2.376, test accuracy 42.684\nEpoch:25\ntrain loss 1.114, test loss 2.400, test accuracy 43.088\nEpoch:26\ntrain loss 1.094, test loss 2.377, test accuracy 43.035\nEpoch:27\ntrain loss 1.066, test loss 2.398, test accuracy 43.070\nEpoch:28\ntrain loss 1.047, test loss 2.404, test accuracy 43.526\nEpoch:29\ntrain loss 1.035, test loss 2.404, test accuracy 43.702\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Training accuracy:{compute_accuracy(model,train_loader)}')\nprint(f'Test_accuracy:{compute_accuracy(model,test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:53:52.032917Z","iopub.execute_input":"2022-11-27T07:53:52.033332Z","iopub.status.idle":"2022-11-27T07:54:00.066452Z","shell.execute_reply.started":"2022-11-27T07:53:52.033297Z","shell.execute_reply":"2022-11-27T07:54:00.065076Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Training accuracy:(0.9737972587521313, tensor(0.7137))\nTest_accuracy:(2.409226936206483, tensor(0.4391))\n","output_type":"stream"}]},{"cell_type":"code","source":"# def load_glove_vectors(glove_file=\"./data/glove.6B/glove.6B.50d.txt\"):\n#     \"\"\"Load the glove word vectors\"\"\"\n#     word_vectors = {}\n#     with open(glove_file) as f:\n#         for line in f:\n#             split = line.split()\n#             word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n#     return word_vectors","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n#     \"\"\" Creates embedding matrix from word vectors\"\"\"\n#     vocab_size = len(word_counts) + 2\n#     vocab_to_idx = {}\n#     vocab = [\"\", \"UNK\"]\n#     W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n#     W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n#     W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n#     vocab_to_idx[\"UNK\"] = 1\n#     i = 2\n#     for word in word_counts:\n#         if word in word_vecs:\n#             W[i] = word_vecs[word]\n#         else:\n#             W[i] = np.random.uniform(-0.25,0.25, emb_size)\n#         vocab_to_idx[word] = i\n#         vocab.append(word)\n#         i += 1   \n#     return W, np.array(vocab), vocab_to_idx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_vecs = load_glove_vectors()\n# pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class LSTM_glove_vecs(torch.nn.Module) :\n#     def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n#         super().__init__()\n#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n#         self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n#         self.embeddings.weight.requires_grad = False ## freeze embeddings\n#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n#         self.linear = nn.Linear(hidden_dim, 5)\n#         self.dropout = nn.Dropout(0.2)\n        \n#     def forward(self, x, l):\n#         x = self.embeddings(x)\n#         x = self.dropout(x)\n#         lstm_out, (ht, ct) = self.lstm(x)\n#         return self.linear(ht[-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)\n# train_model(model, epochs=30, lr=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}